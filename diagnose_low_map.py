#!/usr/bin/env python3
"""
Diagnostic complet pour mAP tr√®s basse (0.02 et 0.004)
V√©rifie: dataset, annotations, d√©s√©quilibre de classes, probl√®mes NMS
"""

import os
import sys
import json
from pathlib import Path
from collections import defaultdict, Counter
import numpy as np

def check_dataset_balance():
    """V√©rifier l'√©quilibre des classes dans le dataset"""
    print("\n" + "="*70)
    print("1Ô∏è‚É£  ANALYSE DES CLASSES - D√âS√âQUILIBRE")
    print("="*70)
    
    dataset_path = Path('dataset')
    labels_train = dataset_path / 'labels' / 'train'
    labels_val = dataset_path / 'labels' / 'val'
    
    class_counts = defaultdict(int)
    class_names = ['helmet', 'glasses', 'person', 'vest', 'boots']
    
    for txt_file in labels_train.glob('*.txt'):
        with open(txt_file, 'r') as f:
            for line in f:
                try:
                    cls = int(line.split()[0])
                    class_counts[cls] += 1
                except:
                    pass
    
    total = sum(class_counts.values())
    print(f"\nüìä Distribution des classes (TRAIN - {total} instances):")
    for cls_id in sorted(class_counts.keys()):
        count = class_counts[cls_id]
        pct = (count / total * 100) if total > 0 else 0
        name = class_names[cls_id] if cls_id < len(class_names) else f"class_{cls_id}"
        print(f"  {name:12} (id={cls_id}): {count:5d} ({pct:5.1f}%)")
    
    if total < 100:
        print(f"\n‚ùå PROBL√àME CRITIQUE: Dataset TR√àS petit ({total} instances)")
        print(f"   Minimum recommand√©: 500+ instances par classe")
        return False
    
    # V√©rifier d√©s√©quilibre
    counts = list(class_counts.values())
    if counts and max(counts) / min(counts) > 10:
        print(f"\n‚ö†Ô∏è  D√âS√âQUILIBRE GRAVE: ratio max/min = {max(counts)/min(counts):.1f}")
        print(f"   Quelques classes sont sous-repr√©sent√©es")
    
    return total >= 100

def check_annotation_quality():
    """V√©rifier la qualit√© des annotations"""
    print("\n" + "="*70)
    print("2Ô∏è‚É£  QUALIT√â DES ANNOTATIONS")
    print("="*70)
    
    dataset_path = Path('dataset')
    labels_train = dataset_path / 'labels' / 'train'
    
    issues = {
        'empty_files': 0,
        'invalid_format': 0,
        'invalid_bbox': 0,
        'missing_labels': 0
    }
    
    total_files = len(list(labels_train.glob('*.txt')))
    
    for txt_file in labels_train.glob('*.txt'):
        with open(txt_file, 'r') as f:
            lines = f.readlines()
        
        if not lines:
            issues['empty_files'] += 1
            continue
        
        for line in lines:
            parts = line.strip().split()
            if len(parts) < 5:
                issues['invalid_format'] += 1
                continue
            
            try:
                cls, x, y, w, h = float(parts[0]), float(parts[1]), float(parts[2]), float(parts[3]), float(parts[4])
                if not (0 <= cls <= 4) or not (0 < w < 1 and 0 < h < 1):
                    issues['invalid_bbox'] += 1
            except:
                issues['invalid_format'] += 1
    
    img_count = len([f for f in (dataset_path / 'images' / 'train').glob('*.*') if f.suffix.lower() not in ['.npy']])
    label_count = total_files
    
    print(f"\nüìã Fichiers:")
    print(f"  Images train: {img_count}")
    print(f"  Labels train: {label_count}")
    
    if img_count != label_count:
        print(f"\n‚ùå MISMATCH: {abs(img_count - label_count)} fichiers sans correspondance")
    
    print(f"\n‚ö†Ô∏è  Probl√®mes trouv√©s:")
    print(f"  Fichiers vides: {issues['empty_files']}")
    print(f"  Format invalide: {issues['invalid_format']}")
    print(f"  Bounding boxes invalides: {issues['invalid_bbox']}")
    
    if issues['empty_files'] > total_files * 0.1:
        print(f"\n‚ùå {issues['empty_files']} fichiers vides ({issues['empty_files']/total_files*100:.1f}%)")
    
    return sum(issues.values()) == 0

def check_data_yaml():
    """V√©rifier le fichier data.yaml"""
    print("\n" + "="*70)
    print("3Ô∏è‚É£  CONFIGURATION DATA.YAML")
    print("="*70)
    
    yaml_path = Path('dataset/data.yaml')
    if not yaml_path.exists():
        print("‚ùå data.yaml introuvable!")
        return False
    
    import yaml
    with open(yaml_path) as f:
        config = yaml.safe_load(f)
    
    print(f"\nClasses (nc): {config.get('nc')}")
    print(f"Noms: {config.get('names')}")
    
    return True

def check_dataset_size_recommendation():
    """Recommandations bas√©es sur la taille du dataset"""
    print("\n" + "="*70)
    print("4Ô∏è‚É£  RECOMMANDATIONS DE TAILLE")
    print("="*70)
    
    dataset_path = Path('dataset')
    img_train = len([f for f in (dataset_path / 'images' / 'train').glob('*.*') if f.suffix.lower() not in ['.npy']])
    
    print(f"\nImages d'entra√Ænement: {img_train}")
    
    recommendations = {
        (0, 50): ("‚ùå CRITIQUE", "< 50 images"),
        (50, 200): ("‚ùå TR√àS FAIBLE", "50-200 images"),
        (200, 500): ("‚ö†Ô∏è  FAIBLE", "200-500 images"),
        (500, 1000): ("‚úÖ BON", "500-1000 images"),
        (1000, 10000): ("‚úÖ EXCELLENT", "1000-10000 images"),
        (10000, float('inf')): ("‚≠ê IDEAL", "10000+ images")
    }
    
    for (min_img, max_img), (emoji, desc) in recommendations.items():
        if min_img <= img_train < max_img:
            print(f"\n{emoji} {desc}")
            if img_train < 500:
                print(f"\n  Actions requises:")
                print(f"  1. Augmenter le dataset (data augmentation)")
                print(f"  2. Collecter plus d'images r√©elles")
                print(f"  3. Utiliser des transforms agressives")

def check_nms_config():
    """V√©rifier la configuration NMS"""
    print("\n" + "="*70)
    print("5Ô∏è‚É£  PROBL√àME NMS TIME LIMIT EXCEEDED")
    print("="*70)
    
    config_path = Path('config.py')
    with open(config_path) as f:
        config_text = f.read()
    
    print("\nProbl√®me d√©tect√©: NMS time limit 2.100s exceeded")
    print("Causes possibles:")
    print("  ‚ùå Trop de d√©tections (mauvaises annotations)")
    print("  ‚ùå Threshold IOU trop bas (0.45)")
    print("  ‚ùå Confidence threshold trop bas (0.25)")
    print("  ‚ùå Multi-model ensemble actif")
    
    print("\n‚úÖ Solutions:")
    print("  1. Augmenter IOU_THRESHOLD: 0.45 ‚Üí 0.65")
    print("  2. Augmenter CONFIDENCE_THRESHOLD: 0.25 ‚Üí 0.5")
    print("  3. D√©sactiver MULTI_MODEL_ENABLED")
    print("  4. R√©duire USE_ENSEMBLE_FOR_CAMERA")

def main():
    print("\n" + "üîç "*20)
    print("DIAGNOSTIC COMPLET - mAP TR√àS BASSE (0.02)")
    print("üîç "*20)
    
    results = []
    results.append(("Dataset Balance", check_dataset_balance()))
    results.append(("Annotation Quality", check_annotation_quality()))
    results.append(("data.yaml Config", check_data_yaml()))
    check_dataset_size_recommendation()
    check_nms_config()
    
    print("\n" + "="*70)
    print("üìã R√âSUM√â DES PROBL√àMES")
    print("="*70)
    
    for name, status in results:
        emoji = "‚úÖ" if status else "‚ùå"
        print(f"{emoji} {name}: {'OK' if status else 'ERREUR'}")
    
    print("\n" + "="*70)
    print("üöÄ ACTIONS √Ä FAIRE IMM√âDIATEMENT")
    print("="*70)
    print("""
1. V√âRIFIER LES DONN√âES
   python diagnose_low_map.py

2. AUGMENTER LE DATASET
   python augment_dataset.py --factor 5

3. CORRIGER NMS
   √âditer config.py:
   - IOU_THRESHOLD = 0.65  (was 0.45)
   - CONFIDENCE_THRESHOLD = 0.5  (was 0.25)

4. R√âENTRA√éNER
   python fast_train.py --epochs 100 --batch 16

5. MONITOR LA VALIDATION
   - Regarder val_loss (doit descendre)
   - V√©rifier mAP50 apr√®s 50 epochs
""")

if __name__ == '__main__':
    main()
