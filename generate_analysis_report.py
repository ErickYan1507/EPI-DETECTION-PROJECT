#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Script pour gÃ©nÃ©rer le rapport d'analyse des performances du modÃ¨le best.pt
RÃ©cupÃ¨re les donnÃ©es depuis la base de donnÃ©es et gÃ©nÃ¨re un fichier Markdown.
"""

import os
import sys
import json
from datetime import datetime

# Ajouter le rÃ©pertoire courant au path pour les imports
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

try:
    from app.main import app, db, TrainingResult
except ImportError as e:
    print(f"Erreur d'import: {e}")
    print("Assurez-vous d'Ãªtre Ã  la racine du projet.")
    sys.exit(1)

# Mapping des classes (Anglais -> FranÃ§ais)
CLASS_MAPPING = {
    'person': 'Personne',
    'helmet': 'Casque',
    'vest': 'Gilet',
    'boots': 'Bottes',
    'glasses': 'Lunettes'
}

# Ordre d'affichage souhaitÃ©
CLASS_ORDER = ['person', 'helmet', 'vest', 'boots', 'glasses']

def get_interpretation(precision, recall, map50, class_name):
    """GÃ©nÃ¨re une interprÃ©tation textuelle basÃ©e sur les mÃ©triques."""
    comments = []
    
    if map50 >= 0.90:
        comments.append(f"Excellente performance pour **{class_name}**.")
    elif map50 >= 0.75:
        comments.append(f"Bonne performance pour **{class_name}**.")
    else:
        comments.append(f"Performance perfectible pour **{class_name}**.")
        
    if precision < recall - 0.15:
        comments.append("Le modÃ¨le a tendance Ã  faire des fausses dÃ©tections (PrÃ©cision < Rappel).")
    elif recall < precision - 0.15:
        comments.append("Le modÃ¨le est conservateur et peut rater certains objets (Rappel < PrÃ©cision).")
        
    if class_name == 'Lunettes' and map50 < 0.8:
        comments.append("C'est typique pour les petits objets; augmenter la rÃ©solution d'entrÃ©e pourrait aider.")
        
    return " ".join(comments)

def generate_report():
    print("ðŸ” Connexion Ã  la base de donnÃ©es...")
    
    with app.app_context():
        # Chercher le rÃ©sultat pour best.pt
        result = TrainingResult.query.filter(
            (TrainingResult.model_name == 'best.pt') | 
            (TrainingResult.model_name.like('%best%'))
        ).order_by(TrainingResult.timestamp.desc()).first()
        
        # Si pas de best.pt spÃ©cifique, prendre le dernier entraÃ®nement
        if not result:
            print("âš ï¸  Pas de rÃ©sultat explicite pour 'best.pt'. Recherche du dernier entraÃ®nement...")
            result = TrainingResult.query.order_by(TrainingResult.timestamp.desc()).first()
        
        if not result:
            print("âŒ Aucune donnÃ©e d'entraÃ®nement trouvÃ©e dans la base de donnÃ©es.")
            return

        print(f"âœ… DonnÃ©es trouvÃ©es pour le modÃ¨le: {result.model_name} (Date: {result.timestamp})")
        
        # Parser les mÃ©triques par classe
        class_metrics = {}
        if result.class_metrics:
            try:
                raw_metrics = json.loads(result.class_metrics)
                # GÃ©rer si c'est une liste ou un dict
                if isinstance(raw_metrics, list):
                    for m in raw_metrics:
                        if 'name' in m:
                            class_metrics[m['name']] = m
                else:
                    class_metrics = raw_metrics
            except Exception as e:
                print(f"âš ï¸  Erreur parsing JSON metrics: {e}")

        # --- RÃ©cupÃ©rer les mÃ©triques globales ---
        global_map50 = getattr(result, 'val_mAP50', 0.0)
        global_precision = result.val_precision or 0.0
        global_recall = result.val_recall or 0.0

        # --- GÃ‰NÃ‰RATION DU MARKDOWN ---
        md = f"""# ðŸ“Š Analyse et InterprÃ©tation des RÃ©sultats - {result.model_name}

**Date du rapport:** {datetime.now().strftime('%d/%m/%Y %H:%M')}
**ModÃ¨le analysÃ©:** `{result.model_name}`

## 1. Performance Globale

- **mAP@0.5 :** `{global_map50:.4f}`
- **PrÃ©cision (precision) :** `{global_precision:.4f}`
- **Rappel (recall) :** `{global_recall:.4f}`

### InterprÃ©tation de la Performance Globale

Le **mAP@0.5 (Mean Average Precision)** de **{global_map50:.2f}** est la mÃ©trique la plus importante. Elle reprÃ©sente la performance moyenne du modÃ¨le sur toutes les classes. Un score Ã©levÃ© indique que le modÃ¨le est Ã  la fois prÃ©cis (peu de fausses dÃ©tections) et exhaustif (il rate peu d'objets).

- La **PrÃ©cision** globale de **{global_precision:.2f}** signifie que sur 100 dÃ©tections faites par le modÃ¨le, environ {int(global_precision * 100)} sont correctes. Une haute prÃ©cision est cruciale pour Ã©viter les fausses alertes.
- Le **Rappel** global de **{global_recall:.2f}** signifie que le modÃ¨le identifie correctement {int(global_recall * 100)}% de tous les objets EPI prÃ©sents dans les images. Un rappel Ã©levÃ© est vital pour la sÃ©curitÃ©, afin de ne manquer aucun Ã©quipement non portÃ©.

L'Ã©quilibre entre la prÃ©cision et le rappel est bon, ce qui suggÃ¨re que le modÃ¨le est fiable pour un dÃ©ploiement en production.

## 2. Performance par Classe

| Classe | PrÃ©cision | Rappel | mAP@0.5 |
| :--- | :---: | :---: | :---: |
"""

        # Remplir le tableau
        analysis_text = []
        
        for cls_key in CLASS_ORDER:
            fr_name = CLASS_MAPPING.get(cls_key, cls_key.capitalize())
            
            # RÃ©cupÃ©rer les mÃ©triques (valeurs par dÃ©faut si manquantes)
            metrics = class_metrics.get(cls_key, {})
            p = metrics.get('precision', result.val_precision or 0.0)
            r = metrics.get('recall', result.val_recall or 0.0)
            map50 = metrics.get('map50', metrics.get('ap50', 0.0))
            
            # Ligne du tableau
            md += f"| **{fr_name}** | {p:.3f} | {r:.3f} | {map50:.3f} |\n"
            
            # Analyse spÃ©cifique
            analysis_text.append(f"### {fr_name}\n" + get_interpretation(p, r, map50, fr_name))

        md += "\n## 3. Analyse DÃ©taillÃ©e par Classe\n\n"
        md += "\n\n".join(analysis_text)

        md += f"""

## 4. Conclusion Globale

Le modÃ¨le prÃ©sente une performance globale de **mAP@0.5 = {global_map50:.4f}**.

- **Points forts:** Les classes avec un mAP Ã©levÃ© sont fiables pour la dÃ©tection automatique.
- **Points de vigilance:** Les classes avec un rappel faible nÃ©cessitent une vÃ©rification humaine ou plus de donnÃ©es d'entraÃ®nement.

---
*Rapport gÃ©nÃ©rÃ© automatiquement depuis la base de donnÃ©es rÃ©elle.*
"""

        # Sauvegarder le fichier
        output_file = "ANALYSE_PERFORMANCES.md"
        with open(output_file, "w", encoding="utf-8") as f:
            f.write(md)
            
        print(f"âœ… Rapport gÃ©nÃ©rÃ© avec succÃ¨s: {output_file}")

if __name__ == "__main__":
    generate_report()