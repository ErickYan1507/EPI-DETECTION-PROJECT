# Monitoring et Maintenance

## ðŸ“Š Monitoring Application

### Health Check Endpoint

```bash
curl http://localhost:5000/api/health
```

Response:
```json
{
  "status": "healthy",
  "uptime_seconds": 3600,
  "database": "connected",
  "model": "loaded",
  "timestamp": "2026-01-09T10:30:45Z"
}
```

### MÃ©triques en Temps RÃ©el

```bash
# CPU Usage
python -c "import psutil; print(f'CPU: {psutil.cpu_percent()}%')"

# Memory Usage
python -c "import psutil; print(f'RAM: {psutil.virtual_memory().percent}%')"

# Disk Usage
python -c "import shutil; print(f'Disk: {shutil.disk_usage(\"/\").percent}%')"
```

### Docker Stats

```bash
docker stats epi-detection-app --no-stream
```

## ðŸ“ˆ Logging

### Configuration

```python
# app/logger.py
import logging
import logging.handlers

def setup_logger(name):
    logger = logging.getLogger(name)
    logger.setLevel(logging.INFO)
    
    # Fichier avec rotation
    handler = logging.handlers.RotatingFileHandler(
        'logs/app.log',
        maxBytes=10485760,  # 10MB
        backupCount=10
    )
    
    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    handler.setFormatter(formatter)
    logger.addHandler(handler)
    
    return logger
```

### Consulter les Logs

```bash
# Dernier 50 lignes
tail -n 50 logs/app.log

# Rechercher erreurs
grep ERROR logs/app.log

# Suivre en temps rÃ©el
tail -f logs/app.log
```

### Rotation Logs

Les logs sont automatiquement archivÃ©s aprÃ¨s 10MB:
```
logs/
â”œâ”€â”€ app.log          # Courant
â”œâ”€â”€ app.log.1        # Archive
â”œâ”€â”€ app.log.2
â””â”€â”€ app.log.3
```

## ðŸ”„ Sauvegarde (Backup)

### AutomatisÃ© avec Cron (Linux)

```bash
# Ã‰diter crontab
crontab -e

# Ajouter (tous les jours Ã  2h du matin)
0 2 * * * /usr/bin/python3 /app/backup_script.py
```

Script backup:
```python
# backup_script.py
import shutil
from datetime import datetime

date = datetime.now().strftime('%Y%m%d_%H%M%S')

# Backup BD
shutil.copy(
    'database/epi_detection.db',
    f'backups/db_{date}.db'
)

# Backup logs
shutil.copytree(
    'logs',
    f'backups/logs_{date}',
    dirs_exist_ok=True
)

print(f"Backup crÃ©Ã©: {date}")
```

### Sauvegarde Manuelle

```bash
# CrÃ©er archive
tar -czf backup_$(date +%Y%m%d).tar.gz \
  database/ logs/ models/best.pt

# Avec cloud (exemple AWS S3)
aws s3 cp backup_$(date +%Y%m%d).tar.gz s3://my-bucket/backups/
```

### Restauration

```bash
# Extraire archive
tar -xzf backup_20260109.tar.gz

# Ou copier fichier unique
cp backups/db_20260109_020000.db database/epi_detection.db
```

## ðŸ” Surveillance Proactive

### Alertes par Email (optionnel)

```python
# app/alerts.py
from flask_mail import Mail, Message

mail = Mail()

def send_alert(subject, body):
    msg = Message(
        subject=subject,
        recipients=['admin@example.com'],
        body=body
    )
    mail.send(msg)
```

Utilisation:
```python
if memory_percent > 90:
    send_alert("Alerte MÃ©moire", f"RAM: {memory_percent}%")
```

### Uptime Monitoring

```bash
# Service systemd (Linux)
sudo systemctl enable epi-detection
sudo systemctl status epi-detection

# VÃ©rifier auto-restart
systemctl show -p RestartForceExitStatus epi-detection
```

## ðŸ“Š MÃ©triques de Base de DonnÃ©es

### Taille BD

```bash
# En bytes
ls -lh database/epi_detection.db

# Via SQLite
sqlite3 database/epi_detection.db "SELECT page_count * page_size FROM pragma_page_count(), pragma_page_size();"
```

### Nombre de DÃ©tections

```bash
sqlite3 database/epi_detection.db "SELECT COUNT(*) FROM detections;"
```

### Nettoyer BD Ancienne

```python
# Supprimer dÃ©tections > 90 jours
from datetime import datetime, timedelta
from app.database import Detection

cutoff = datetime.utcnow() - timedelta(days=90)
old_detections = Detection.query.filter(Detection.timestamp < cutoff).delete()
db.session.commit()

print(f"SupprimÃ© {old_detections} dÃ©tections")
```

## ðŸš€ Optimisation Continue

### Analyser les Performances

```python
# CrÃ©er rapport performance
from app.database import Detection, Session
from sqlalchemy import func

# DÃ©tections par heure
hourly = db.session.query(
    func.strftime('%H', Detection.timestamp).label('hour'),
    func.count(Detection.id).label('count')
).group_by('hour').all()

for hour, count in hourly:
    print(f"Heure {hour}: {count} dÃ©tections")
```

### Identifier les Goulots

```bash
# Profiler CPU
python -m cProfile -s cumtime app/main.py > profile.txt

# VÃ©rifier imports lents
python -X importtime app/main.py 2> import_log.txt
```

## ðŸ“‹ Checklist Maintenance RÃ©guliÃ¨re

### Hebdomadaire
- [ ] VÃ©rifier logs pour erreurs
- [ ] Confirmer health check passant
- [ ] VÃ©rifier espace disque

### Mensuelle
- [ ] ExÃ©cuter backup
- [ ] VÃ©rifier taille BD
- [ ] Nettoyer logs anciens
- [ ] Tester restauration backup

### Trimestriellement
- [ ] Mettre Ã  jour dÃ©pendances Python
- [ ] RÃ©optimiser BD (VACUUM)
- [ ] Archiver dÃ©tections anciennes
- [ ] Analyser mÃ©triques usage

## ðŸ”— Ressources

- [PostgreSQL Monitoring](https://www.postgresql.org/docs/current/monitoring.html)
- [SQLite Best Practices](https://www.sqlite.org/bestpractice.html)
- [Python Logging](https://docs.python.org/3/library/logging.html)
- [Flask-SQLAlchemy](https://flask-sqlalchemy.palletsprojects.com/)
